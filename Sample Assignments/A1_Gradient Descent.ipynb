{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Assignment One (EECS3405 F25)**\n",
        "\n",
        "**Simon Posluns (221541727):**    \n",
        "\n",
        "**simonp22@my.yorku.ca**\n",
        "\n",
        "This assignment is mainly for you to review mathematical background. You have to work individually. Remember to fill in your information (name, student number, email) at above.\n",
        "\n",
        "\n",
        "\n",
        "##**What to Submit**\n",
        "\n",
        "Please use this notebook to complete assignment one. You have to run your codes and show the results in this notebook. Download the completed notenook as `.ipynb` and compress it as a `.zip` file to submit to eClass.  Submit only ONE notebook file that contains all of your answers and codes to eClass before the deadline.  No late submission\n",
        "will be accepted.\n",
        "\n",
        "* For all written parts, write your answers in text cells. To avoid confusions in marking, better to embed latex codes there to represent all mathematical notations and equations.  **No handwriting is accepted**.\n",
        "\n",
        "* For programming parts, you should give codes, comments, explanations and the proper running outputs in both code and text cells. Make your jupyter notebook clean and concise. Remove all unused codes and all intermediate results from the submitted notebook. The submitted notebook should include only the final (best) outputs for each question. Also make sure every code cell runnable so that markers can reproduce the outputs if necessary."
      ],
      "metadata": {
        "id": "3EU9nI8EWbrS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1** (10 marks)  ###\n",
        "\n",
        "Given two sets of $m$ vectors, $\\mathbf{x}_i \\in \\mathbb{R}^n$  and $\\mathbf{y}_i \\in \\mathbb{R}^n$ for all $i=1,2, \\cdots, m$, verify that the summation $\\sum_{i=1}^m  \\mathbf{x}_i  \\mathbf{y}_i^\\intercal$ can be vectorized as the following matrix multiplication:\n",
        "$$\n",
        "\\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^\\intercal = \\mathbf{X}  \\mathbf{Y}^\\intercal,\n",
        "$$\n",
        "where $\\mathbf{X} = \\big[ \\mathbf{x}_1 \\, \\mathbf{x}_2 \\, \\cdots \\, \\mathbf{x}_m \\big] \\in \\mathbb{R}^{n\\times m}$ and $\\mathbf{Y} = \\big[ \\mathbf{y}_1 \\, \\mathbf{y}_2 \\, \\cdots \\, \\mathbf{y}_m \\big] \\in \\mathbb{R}^{n\\times m}$."
      ],
      "metadata": {
        "id": "E1OwmDh6Zv_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1 Response:** Where X and Y are sets of vectors as stated above, we start from the RHS: $XY^T =\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_m\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{y}_1^\\top \\\\\n",
        "\\mathbf{y}_2^\\top \\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf{y}_m^\\top\n",
        "\\end{bmatrix}\n",
        "= \\mathbf{x}_1 \\mathbf{y}_1^\\top\n",
        "+ \\mathbf{x}_2 \\mathbf{y}_2^\\top\n",
        "+ \\cdots\n",
        "+ \\mathbf{x}_m \\mathbf{y}_m^\\top = \\sum_{i=1}^m \\mathbf{x}_i \\mathbf{y}_i^\\top.\n",
        "$"
      ],
      "metadata": {
        "id": "rIWKLT7taeil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q2** (10 marks)  ###\n",
        "\n",
        "Given $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{z} \\in \\mathbb{R}^m$, and\n",
        "$\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ ($m < n$), compute the following two partial derivatives:\n",
        "\n",
        "1.   $\\frac{\\partial}{\\partial \\mathbf{z}} \\, \\Vert  \\mathbf{A} \\mathbf{x} - \\mathbf{z} \\Vert^2$\n",
        "\n",
        "2.   $\\frac{\\partial}{\\partial \\mathbf{x}} \\, \\Vert  \\mathbf{A} \\mathbf{x} - \\mathbf{z} \\Vert^2$\n",
        "\n"
      ],
      "metadata": {
        "id": "RgoTthU7aoV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2 Response:**\n",
        "\n",
        "1.    $\\frac{\\partial}{\\partial \\mathbf{z}} \\, \\Vert  \\mathbf{A} \\mathbf{x} - \\mathbf{z} \\Vert^2$ = $\\frac{\\partial}{\\partial \\mathbf{z}} \\, [(  \\mathbf{A} \\mathbf{x} - \\mathbf{z} )(  \\mathbf{A} \\mathbf{x} - \\mathbf{z} )^T ]$  \n",
        "= $\\frac{\\partial}{\\partial \\mathbf{z}} \\ \\sum_{i=1}^m \\big( \\mathbf{A}\\mathbf{x}_i - \\mathbf{z}_i \\big)^2 $  \n",
        "\n",
        "= $2\\big( \\mathbf{A}\\mathbf{x} - \\mathbf{z} \\big)⋅\\frac{\\partial}{\\partial\\mathbf{z}}\\big( \\mathbf{A}\\mathbf{x} - \\mathbf{z} \\big) $  \n",
        "\n",
        "= $2\\big( \\mathbf{A}\\mathbf{x} - \\mathbf{z} \\big)⋅-1 $  \n",
        "\n",
        "= $2\\big( \\mathbf{z} -\\mathbf{A}\\mathbf{x} \\big) $  \n",
        "  \n",
        "\n",
        "2.  $\\frac{\\partial}{\\partial \\mathbf{x}} \\, \\Vert  \\mathbf{A} \\mathbf{x} - \\mathbf{z} \\Vert^2$ = $\\frac{\\partial}{\\partial \\mathbf{z}} \\, [(  \\mathbf{A} \\mathbf{x} - \\mathbf{z} )(  \\mathbf{A} \\mathbf{x} - \\mathbf{z} )^T ]$  \n",
        "= $\\frac{\\partial}{\\partial \\mathbf{x}} \\ \\sum_{i=1}^m \\big( \\mathbf{A}\\mathbf{x}_i - \\mathbf{z}_i \\big)^2 $\n",
        "    \n",
        "= $2\\big( \\mathbf{A}\\mathbf{x} - z \\big)^T⋅\\frac{\\partial}{\\partial\\mathbf{x}}\\big( \\mathbf{A}\\mathbf{x} - \\mathbf{z} \\big) $  \n",
        "\n",
        "= $2\\big( \\mathbf{A}\\mathbf{x} - \\mathbf{z} \\big)^T⋅(A-0) $  \n",
        "\n",
        "= $2\\big( \\mathbf{A}\\mathbf{x} - \\mathbf{z} \\big)^T⋅A $\n",
        "\n",
        "= $2A^T\\big( \\mathbf{A}\\mathbf{x} - \\mathbf{z} \\big) $"
      ],
      "metadata": {
        "id": "kId5VtjBgTAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q3** (20 marks) ###\n",
        "\n",
        "**Part 3.1 (10 marks):** Derive a gradient decent algorithm to solve the following quadratic optimization problem:\n",
        "\n",
        "$$\n",
        "    \\min_{\\mathbf{x}} \\;\\; \\frac12 \\mathbf{x}^\\intercal \\mathbf{Q} \\mathbf{x} - \\mathbf{1}^\\intercal \\mathbf{x}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{x} \\in \\mathbb{R}^n$ is free variables, $\\mathbf{1}$ is a constant $n$-dimensional vector whose elements are all 1,  and $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ is given as a constant definite positive matrix ($\\mathbf{Q} \\succ 0$)."
      ],
      "metadata": {
        "id": "2jw48ZfUexHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.1 Response:**\n",
        "\n",
        "\n",
        "$$ \\nabla f(\\mathbf{x}) = \\frac{\\partial}{\\partial \\mathbf{x}} \\Big( \\frac{1}{2} \\mathbf{x}^\\intercal \\mathbf{Q} \\mathbf{x} - \\mathbf{1}^\\intercal \\mathbf{x} \\Big)  $$\n",
        "\n",
        "Since Q is a constant definite positive matrix, we know it is symmetric, thus:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial \\mathbf{x}} \\Big( \\frac{1}{2} \\mathbf{x}^\\intercal \\mathbf{Q} \\mathbf{x} \\Big) = \\mathbf{Q} \\mathbf{x} $$\n",
        "  \n",
        "$$\\frac{\\partial}{\\partial \\mathbf{x}} \\Big( -\\mathbf{1}^\\intercal \\mathbf{x} \\Big) = - \\mathbf{1}$$  \n",
        "\n",
        "Gradient:$ \\nabla f(\\mathbf{x}) = \\mathbf{Q} \\mathbf{x} - \\mathbf{1}$\n",
        "\n",
        "Initialization and step size, starting at x_0:    \n",
        "$$\\mathbf{x}_0 \\in \\mathbb{R}^n, \\quad \\text{choose step size } \\alpha \\in \\left(0, \\frac{2}{\\lambda_{\\max}(\\mathbf{Q})}\\right)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Iterative algorithm: for each step k, the gradient at that point is computed and subsequent step (k+1) is in the direction of steepest decent (where k = 0,1,2,3 ...):\n",
        "$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k) \\\\\n",
        "= \\mathbf{x}_k - \\alpha (\\mathbf{Q}{x}_k - \\mathbf{1}) \\\\\n",
        "= \\mathbf{x}_k - \\alpha\\mathbf{Q}{x}_k - \\alpha\\mathbf{1}\\\\\n",
        "= (\\mathbf{I} - \\alpha \\mathbf{Q}) \\mathbf{x}_k + \\alpha \\mathbf{1} \\text{  *Where I signifies identity matrix}$$\n",
        "\n",
        "\n",
        "Convergence at minimum(ɛ is used for a small tolerance):\n",
        "\n",
        "$$\\text{Stop if } \\|\\mathbf{x}_{k+1} - \\mathbf{x}_k\\| < \\epsilon \\text{  or} \\quad \\|\\nabla f(\\mathbf{x}_k)\\| < \\epsilon $$\n",
        "\n",
        "Minimum is reached if the difference between x_k and x_k+1 is less than ɛ, or the gradient is less thn ɛ (approaches 0).\n",
        "\n",
        "  Closed form solution, setting ∇f(x) = 0:\n",
        "$$\n",
        "\\mathbf{Q} \\mathbf{x}^* - \\mathbf{1} = 0 \\\\\n",
        "\\mathbf{Q} \\mathbf{x}^* = \\mathbf{1} \\\\\n",
        "\\mathbf{x}^* = \\mathbf{Q}^{-1} \\mathbf{1} \\quad (\\text{since } \\mathbf{Q} \\succ 0 \\text{ and is invertible})\n",
        "$$\n"
      ],
      "metadata": {
        "id": "dXRhRXQCfLLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 3.2 (10 marks):**  Use `numpy` to write a gradient descent code to solve the above quadratic optimization problem.  Print the solution $\\mathbf{x}^*$ for the following $\\mathbf{Q}$ matrix."
      ],
      "metadata": {
        "id": "z3mUDnDlfchg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "Q = np.array([[13.57599411,-7.77916099,4.45073215,0.18157029,2.55400279,-1.06271637,-0.14237939,-4.73200647],\n",
        " [-7.77916099,9.16482133,1.349321,0.4697293,-3.29959602,3.41477056,-4.66924506,1.74397411],\n",
        " [4.45073215,1.349321,11.66150687,4.7431563,4.6278791,0.61248024,-2.52679826,-4.94462079],\n",
        " [0.18157029,0.4697293,4.7431563,13.50647727,8.16622632,2.77908368,-2.36985429,-7.53962758],\n",
        " [2.55400279,-3.29959602,4.6278791,8.16622632,12.2258951,-1.84304299,1.04977205,-8.24939242],\n",
        " [-1.06271637,3.41477056,0.61248024,2.77908368,-1.84304299,6.67184174,-4.60482886,0.28920379],\n",
        " [-0.14237939,-4.66924506,-2.52679826,-2.36985429,1.04977205,-4.60482886,13.16925778,3.41548824],\n",
        " [-4.73200647,1.74397411,-4.94462079, -7.53962758,-8.24939242,0.28920379,3.41548824,8.72679051]])\n",
        "\n",
        "print(Q.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPl-IOvzemCa",
        "outputId": "38d86d40-f46e-4838-ab81-47527de12f43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "Q = np.array([[13.57599411,-7.77916099,4.45073215,0.18157029,2.55400279,-1.06271637,-0.14237939,-4.73200647],\n",
        "              [-7.77916099,9.16482133,1.349321,0.4697293,-3.29959602,3.41477056,-4.66924506,1.74397411],\n",
        "              [4.45073215,1.349321,11.66150687,4.7431563,4.6278791,0.61248024,-2.52679826,-4.94462079],\n",
        "              [0.18157029,0.4697293,4.7431563,13.50647727,8.16622632,2.77908368,-2.36985429,-7.53962758],\n",
        "              [2.55400279,-3.29959602,4.6278791,8.16622632,12.2258951,-1.84304299,1.04977205,-8.24939242],\n",
        "              [-1.06271637,3.41477056,0.61248024,2.77908368,-1.84304299,6.67184174,-4.60482886,0.28920379],\n",
        "              [-0.14237939,-4.66924506,-2.52679826,-2.36985429,1.04977205,-4.60482886,13.16925778,3.41548824],\n",
        "              [-4.73200647,1.74397411,-4.94462079,-7.53962758,-8.24939242,0.28920379,3.41548824,8.72679051]])\n",
        "\n",
        "n = Q.shape[0]    # extract number of variables in Q\n",
        "\n",
        "# Vector of ones which is of length n\n",
        "one_vec = np.ones(n)\n",
        "\n",
        "# Initialize x\n",
        "x = np.zeros(n)\n",
        "\n",
        "# size of step corresponding to alpha\n",
        "alpha = 0.01\n",
        "\n",
        "# Maximum number iterations\n",
        "max = 10000\n",
        "#tolerance\n",
        "epsilon = 1e-6\n",
        "\n",
        "for k in range(max):\n",
        "    grad = Q @ x - one_vec         # compute gradient\n",
        "    x_new = x - alpha * grad       # x_new is the next x_k+1 value\n",
        "\n",
        "    # check for convergence: if either then change in x OR the gradient is less than epsilon\n",
        "    if np.linalg.norm(x_new - x) < epsilon or np.linalg.norm(grad) < epsilon:\n",
        "        x = x_new\n",
        "        break\n",
        "    x = x_new\n",
        "    alpha = 0.999999 * alpha     # Decay the step size with each iteration\n",
        "\n",
        "print(\"Gradient descent solution x:\\n\", x)\n",
        "\n",
        "# Optional: exact solution for verification\n",
        "# by setting gradient to 0, we found that x^* equals the inverse of Q multiplied by the 1 vector\n",
        "x_exact = np.linalg.inv(Q) @ one_vec\n",
        "print(\"\\nExact solution x:\\n\", x_exact)\n",
        "\n",
        "abs_diff = np.abs(x - x_exact)\n",
        "print(\"\\nAbsolute difference |x - x_exact|:\\n\", abs_diff)"
      ],
      "metadata": {
        "id": "QoKi0-TNgHAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76fa233e-9d26-40d9-8625-3ffd9720bdcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient descent solution x:\n",
            " [ 6.90025584  6.97562705 -2.98888261  2.96794633  5.45264147 -3.04951248\n",
            " -1.27318161  9.08655345]\n",
            "\n",
            "Exact solution x:\n",
            " [ 6.90071481  6.97608817 -2.98909008  2.96815218  5.45299981 -3.04974396\n",
            " -1.27328452  9.08716396]\n",
            "\n",
            "Absolute difference |x - x_exact|:\n",
            " [0.00045897 0.00046112 0.00020747 0.00020585 0.00035835 0.00023148\n",
            " 0.00010291 0.00061051]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q4** (20 marks)\n",
        "\n",
        "**Part 4.1 (10 marks):** Derive the procedure to compute the distance of a point $\\mathbf{x}_0 \\in \\mathbb{R}^n$ to\n",
        "an elliptic surface $\\mathbf{x}^\\intercal \\mathbf{A} \\mathbf{x} = 1$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite ($\\mathbf{A} \\succ 0$).\n",
        "\n",
        "*Hints: Use the method of Lagrange multipliers to derive the formula, and may need to use a numerical procedure (e.g. grid search) to find out the unknown multiplier(s) if no closed-form solution exists.*\n",
        "\n"
      ],
      "metadata": {
        "id": "Od0psVpKgX8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1 Response**\n",
        "\n",
        " $\\mathbf{x}^\\intercal \\mathbf{A} \\mathbf{x} = 1$  \n",
        "\n",
        "\n",
        "\n",
        " $$  \\text{Distance function (squared distance):}\\\\\n",
        "d^2 = \\|\\mathbf{x} - \\mathbf{x}_0\\|^2 $$\n",
        "\n",
        "$$\\text{Constraint (ellipsoid surface):} \\\\\n",
        " \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} - 1 = 0\n",
        "$$\n",
        "\n",
        "Thus:   \n",
        "\n",
        "$ ∇(\\|\\mathbf{x} - \\mathbf{x}_0\\|^2) = λ ∇(\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} - 1)$\n",
        "\n",
        "$ ∇ \\|\\mathbf{x} - \\mathbf{x}_0\\|^2 = ∇((\\mathbf{x} - \\mathbf{x}_0))^\\top (\\mathbf{x} - \\mathbf{x}_0)   $\n",
        "\n",
        "$ =∇ \\big[ \\mathbf{x}^\\top \\mathbf{x} - \\mathbf{x}^\\top \\mathbf{x}_0 - \\mathbf{x}_0^\\top \\mathbf{x} + \\mathbf{x}_0^\\top \\mathbf{x}_0 \\big] $\n",
        "\n",
        "$ =∇ \\big[ \\mathbf{x}^\\top \\mathbf{x} -2 \\mathbf{x}^\\top \\mathbf{x}_0 + \\mathbf{x}_0^\\top \\mathbf{x}_0 \\big] $\n",
        "\n",
        "$ = 2x  -2\\mathbf{x}_0 $\n",
        "\n",
        "$∇(\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} - 1) = 2Ax \\quad \\text{(as A is symmetric)}  $\n",
        "\n",
        "\n",
        "Applying Lagrange's Theorem:\n",
        "\n",
        "$2(x  -\\mathbf{x}_0) = λ(2Ax) \\\\\n",
        "x -\\mathbf{x}_0 = λAx  \\\\\n",
        "(x - \\lambda \\mathbf{A}x) = \\mathbf{x}_0 \\\\\n",
        "(\\mathbf{I} - \\lambda \\mathbf{A}) \\mathbf{x} = \\mathbf{x}_0 \\\\\n",
        "\\mathbf{x}^* = (\\mathbf{I} - \\lambda^* \\mathbf{A})^{-1} \\mathbf{x}_0$\n",
        "\n",
        "Substituting into the constraint:\n",
        "\n",
        "$$ \\Big( (\\mathbf{I} - \\lambda^* \\mathbf{A})^{-1} \\mathbf{x}_0 \\Big)^\\top\n",
        "\\mathbf{A} \\Big( (\\mathbf{I} - \\lambda^* \\mathbf{A})^{-1} \\mathbf{x}_0 \\Big) - 1 = 0 $$  \n",
        "\n",
        "\n",
        "\n",
        "**General Numerical Solution for $\\lambda^*$:**\n",
        "\n",
        "Define the function (with f(λ) = 0):\n",
        "\n",
        "$$\n",
        "f(\\lambda) = 0 = \\big((\\mathbf{I} - \\lambda \\mathbf{A})^{-1} \\mathbf{x}_0\\big)^\\top\n",
        "\\mathbf{A} \\big((\\mathbf{I} - \\lambda \\mathbf{A})^{-1} \\mathbf{x}_0\\big) - 1\n",
        "$$\n",
        "\n",
        "Compute its derivative:\n",
        "\n",
        "$$\n",
        "f’(\\lambda) = 2\\mathbf{y}^\\top \\mathbf{A} (\\mathbf{I} - \\lambda \\mathbf{A})^{-1} \\mathbf{A}\n",
        "$$\n",
        "\n",
        "Newton method:\n",
        "\n",
        "$$\n",
        "\\lambda_{k+1} = \\lambda_k - \\frac{f(\\lambda_k)}{f’(\\lambda_k)}\n",
        "$$\n",
        "\n",
        "Repeat until:\n",
        "\n",
        "$$\n",
        "|f(\\lambda_k)| < \\text{tolerance}\n",
        "$$\n",
        "\n",
        "Once converged, compute the closest point:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}^* = (\\mathbf{I} - \\lambda^* \\mathbf{A})^{-1} \\mathbf{x}_0\n",
        "$$\n",
        "\n",
        "Distance:\n",
        "\n",
        "$$\n",
        "d = |\\mathbf{x}^* - \\mathbf{x}_0|\n",
        "$$"
      ],
      "metadata": {
        "id": "7kDdMYE9gp2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4.2 (10 marks):** Use `numpy` to implement a grid search to compute the above distance.\n",
        "Use the following given $\\mathbf{x}_0$ and $\\mathbf{A}$ as an example to print out the distance."
      ],
      "metadata": {
        "id": "9cwqbh2ugrgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import root_scalar\n",
        "\n",
        "A = np.array([[10.82133645,3.92407517,0.28991746,-2.03411727,4.55376106],\n",
        " [3.92407517,8.55336068,-0.21915151,0.42212004,2.08124307],\n",
        " [0.28991746,-0.21915151,5.74622299,-0.800496,0.63366946],\n",
        " [-2.03411727,0.42212004,-0.800496,3.91232017,-0.44196245],\n",
        " [4.55376106,2.08124307,0.63366946,-0.44196245,3.83040884]])\n",
        "\n",
        "x0 = np.array([-0.08111786,1.68122205,0.84108852,1.74916213,-0.11266981])\n",
        "\n",
        "print(\"A.shape = \", A.shape)\n",
        "print(\"x0.shape = \", x0.shape)\n",
        "\n",
        "def f(lam):\n",
        "    I = np.eye(A.shape[0])\n",
        "    x = np.linalg.solve(I - lam*A, x0)\n",
        "    return x.T @ A @ x - 1\n",
        "\n",
        "lam_values = np.linspace(-1.0, 1.0, 100000)\n",
        "f_vals = []\n",
        "\n",
        "for lam in lam_values:\n",
        "    try:\n",
        "        val = f(lam)\n",
        "        f_vals.append(np.abs(val))\n",
        "    except np.linalg.LinAlgError:\n",
        "        f_vals.append(np.inf)\n",
        "\n",
        "f_vals = np.array(f_vals)\n",
        "idx_min = np.argmin(f_vals)\n",
        "lambda_star = lam_values[idx_min]\n",
        "\n",
        "# Closest point and distance\n",
        "x_star = np.linalg.solve(np.eye(A.shape[0]) - lambda_star*A, x0)\n",
        "distance = np.linalg.norm(x_star - x0)\n",
        "\n",
        "print(\"Grid Search Results:\")\n",
        "print(\"lambda* =\", lambda_star)\n",
        "print(\"x* =\", x_star)\n",
        "print(\"distance =\", distance)\n",
        "print(\"Ellipsoid constraint (≈1):\", x_star.T @ A @ x_star)\n",
        "print()\n",
        "\n",
        "def fprime(lam):\n",
        "    I = np.eye(A.shape[0])\n",
        "    inv = np.linalg.inv(I - lam*A)\n",
        "    x = inv @ x0\n",
        "    return 2 * x.T @ A @ (inv @ (A @ x))\n",
        "\n",
        "# Apply Newton’s method for finding roots\n",
        "sol = root_scalar(f, fprime=fprime, x0=0.0, method='newton')\n",
        "lambda_newton = sol.root\n",
        "\n",
        "# Apply Newton’s method for finding roots\n",
        "x_star_newton = np.linalg.solve(np.eye(A.shape[0]) - lambda_newton*A, x0)\n",
        "distance_newton = np.linalg.norm(x_star_newton - x0)\n",
        "\n",
        "print(\"Newton's Method Results:\")\n",
        "print(\"lambda* =\", lambda_newton)\n",
        "print(\"x* =\", x_star_newton)\n",
        "print(\"distance =\", distance_newton)\n",
        "print(\"Ellipsoid constraint (≈1):\", x_star_newton.T @ A @ x_star_newton)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97-9gK9shLMy",
        "outputId": "c4e887c8-d8db-4fd3-f2f6-28ea7eec068f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A.shape =  (5, 5)\n",
            "x0.shape =  (5,)\n",
            "Grid Search Results:\n",
            "lambda* = -0.93219932199322\n",
            "x* = [ 0.0476721   0.18436309  0.19580546  0.39958587 -0.13646972]\n",
            "distance = 2.120256195365985\n",
            "Ellipsoid constraint (≈1): 1.0000001294457894\n",
            "\n",
            "Newton's Method Results:\n",
            "lambda* = -0.9321993974452482\n",
            "x* = [ 0.0476721   0.18436308  0.19580545  0.39958585 -0.13646972]\n",
            "distance = 2.1202562238222824\n",
            "Ellipsoid constraint (≈1): 0.9999999999999997\n"
          ]
        }
      ]
    }
  ]
}