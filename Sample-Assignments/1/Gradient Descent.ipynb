{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EU9nI8EWbrS"
      },
      "source": [
        "## **Gradient Descent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jw48ZfUexHP"
      },
      "source": [
        "A gradient decent algorithm to solve the following quadratic optimization problem can be derived as follows:\n",
        "\n",
        "$$\n",
        "    \\min_{\\mathbf{x}} \\;\\; \\frac12 \\mathbf{x}^\\intercal \\mathbf{Q} \\mathbf{x} - \\mathbf{1}^\\intercal \\mathbf{x}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{x} \\in \\mathbb{R}^n$ is free variables, $\\mathbf{1}$ is a constant $n$-dimensional vector whose elements are all 1,  and $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ is given as a constant definite positive matrix ($\\mathbf{Q} \\succ 0$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXRhRXQCfLLM"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "$$ \\nabla f(\\mathbf{x}) = \\frac{\\partial}{\\partial \\mathbf{x}} \\Big( \\frac{1}{2} \\mathbf{x}^\\intercal \\mathbf{Q} \\mathbf{x} - \\mathbf{1}^\\intercal \\mathbf{x} \\Big)  $$\n",
        "\n",
        "Since Q is a constant definite positive matrix, we know it is symmetric, thus:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial \\mathbf{x}} \\Big( \\frac{1}{2} \\mathbf{x}^\\intercal \\mathbf{Q} \\mathbf{x} \\Big) = \\mathbf{Q} \\mathbf{x} $$\n",
        "  \n",
        "$$\n",
        "\\frac{\\partial}{\\partial \\mathbf{x}} \\big( -\\mathbf{1}^\\top \\mathbf{x} \\big) = -\\mathbf{1}\n",
        "$$\n",
        "\n",
        "Gradient: $\\nabla f(\\mathbf{x}) = \\mathbf{Q} \\mathbf{x} - \\mathbf{1}$\n",
        "\n",
        "Initialization and step size, starting at x_0:\n",
        "\n",
        "$$\\mathbf{x}_0 \\in \\mathbb{R}^n, \\quad \\text{choose step size } \\alpha \\in \\left(0, \\frac{2}{\\lambda_{\\max}(\\mathbf{Q})}\\right) $$\n",
        "\n",
        "\n",
        "Iterative algorithm: for each step k, the gradient at that point is computed and subsequent step (k+1) is in the direction of steepest decent (where k = 0,1,2,3 ...):\n",
        "\n",
        "$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha \\nabla f(\\mathbf{x}_k) \\\\\n",
        "= \\mathbf{x}_k - \\alpha (\\mathbf{Q}{x}_k - \\mathbf{1}) \\\\\n",
        "= \\mathbf{x}_k - \\alpha\\mathbf{Q}{x}_k - \\alpha\\mathbf{1}\\\\\n",
        "= (\\mathbf{I} - \\alpha \\mathbf{Q}) \\mathbf{x}_k + \\alpha \\mathbf{1} \\text{  *Where I signifies identity matrix}$$\n",
        "\n",
        "\n",
        "Convergence at minimum(ɛ is used for a small tolerance):\n",
        "\n",
        "$$\\text{Stop if } \\|\\mathbf{x}_{k+1} - \\mathbf{x}_k\\| < \\epsilon \\text{  or} \\quad \\|\\nabla f(\\mathbf{x}_k)\\| < \\epsilon $$\n",
        "\n",
        "Minimum is reached if the difference between x_k and x_k+1 is less than ɛ, or the gradient is less thn ɛ (approaches 0).\n",
        "\n",
        "  Closed form solution, setting ∇f(x) = 0:\n",
        "$$\n",
        "\\mathbf{Q} \\mathbf{x}^* - \\mathbf{1} = 0 \\\\\n",
        "\\mathbf{Q} \\mathbf{x}^* = \\mathbf{1} \\\\\n",
        "\\mathbf{x}^* = \\mathbf{Q}^{-1} \\mathbf{1} \\quad (\\text{since } \\mathbf{Q} \\succ 0 \\text{ and is invertible})\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3mUDnDlfchg"
      },
      "source": [
        "I implemented a gradient descent algorithm using numpy to solve the quadratic optimization problem described above. Using the provided \\mathbf{Q} matrix, I iteratively updated the solution \\mathbf{x} until convergence. The final solution, denoted as \\mathbf{x}^*, was computed and printed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPl-IOvzemCa",
        "outputId": "38d86d40-f46e-4838-ab81-47527de12f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8, 8)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "Q = np.array([[13.57599411,-7.77916099,4.45073215,0.18157029,2.55400279,-1.06271637,-0.14237939,-4.73200647],\n",
        " [-7.77916099,9.16482133,1.349321,0.4697293,-3.29959602,3.41477056,-4.66924506,1.74397411],\n",
        " [4.45073215,1.349321,11.66150687,4.7431563,4.6278791,0.61248024,-2.52679826,-4.94462079],\n",
        " [0.18157029,0.4697293,4.7431563,13.50647727,8.16622632,2.77908368,-2.36985429,-7.53962758],\n",
        " [2.55400279,-3.29959602,4.6278791,8.16622632,12.2258951,-1.84304299,1.04977205,-8.24939242],\n",
        " [-1.06271637,3.41477056,0.61248024,2.77908368,-1.84304299,6.67184174,-4.60482886,0.28920379],\n",
        " [-0.14237939,-4.66924506,-2.52679826,-2.36985429,1.04977205,-4.60482886,13.16925778,3.41548824],\n",
        " [-4.73200647,1.74397411,-4.94462079, -7.53962758,-8.24939242,0.28920379,3.41548824,8.72679051]])\n",
        "\n",
        "print(Q.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoKi0-TNgHAy",
        "outputId": "76fa233e-9d26-40d9-8625-3ffd9720bdcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient descent solution x:\n",
            " [ 6.90025584  6.97562705 -2.98888261  2.96794633  5.45264147 -3.04951248\n",
            " -1.27318161  9.08655345]\n",
            "\n",
            "Exact solution x:\n",
            " [ 6.90071481  6.97608817 -2.98909008  2.96815218  5.45299981 -3.04974396\n",
            " -1.27328452  9.08716396]\n",
            "\n",
            "Absolute difference |x - x_exact|:\n",
            " [0.00045897 0.00046112 0.00020747 0.00020585 0.00035835 0.00023148\n",
            " 0.00010291 0.00061051]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "Q = np.array([[13.57599411,-7.77916099,4.45073215,0.18157029,2.55400279,-1.06271637,-0.14237939,-4.73200647],\n",
        "              [-7.77916099,9.16482133,1.349321,0.4697293,-3.29959602,3.41477056,-4.66924506,1.74397411],\n",
        "              [4.45073215,1.349321,11.66150687,4.7431563,4.6278791,0.61248024,-2.52679826,-4.94462079],\n",
        "              [0.18157029,0.4697293,4.7431563,13.50647727,8.16622632,2.77908368,-2.36985429,-7.53962758],\n",
        "              [2.55400279,-3.29959602,4.6278791,8.16622632,12.2258951,-1.84304299,1.04977205,-8.24939242],\n",
        "              [-1.06271637,3.41477056,0.61248024,2.77908368,-1.84304299,6.67184174,-4.60482886,0.28920379],\n",
        "              [-0.14237939,-4.66924506,-2.52679826,-2.36985429,1.04977205,-4.60482886,13.16925778,3.41548824],\n",
        "              [-4.73200647,1.74397411,-4.94462079,-7.53962758,-8.24939242,0.28920379,3.41548824,8.72679051]])\n",
        "\n",
        "n = Q.shape[0]    # extract number of variables in Q\n",
        "\n",
        "# Vector of ones which is of length n\n",
        "one_vec = np.ones(n)\n",
        "\n",
        "# Initialize x\n",
        "x = np.zeros(n)\n",
        "\n",
        "# size of step corresponding to alpha\n",
        "alpha = 0.01\n",
        "\n",
        "# Maximum number iterations\n",
        "max = 10000\n",
        "#tolerance\n",
        "epsilon = 1e-6\n",
        "\n",
        "for k in range(max):\n",
        "    grad = Q @ x - one_vec         # compute gradient\n",
        "    x_new = x - alpha * grad       # x_new is the next x_k+1 value\n",
        "\n",
        "    # check for convergence: if either then change in x OR the gradient is less than epsilon\n",
        "    if np.linalg.norm(x_new - x) < epsilon or np.linalg.norm(grad) < epsilon:\n",
        "        x = x_new\n",
        "        break\n",
        "    x = x_new\n",
        "    alpha = 0.999999 * alpha     # Decay the step size with each iteration\n",
        "\n",
        "print(\"Gradient descent solution x:\\n\", x)\n",
        "\n",
        "# Optional: exact solution for verification\n",
        "# by setting gradient to 0, we found that x^* equals the inverse of Q multiplied by the 1 vector\n",
        "x_exact = np.linalg.inv(Q) @ one_vec\n",
        "print(\"\\nExact solution x:\\n\", x_exact)\n",
        "\n",
        "abs_diff = np.abs(x - x_exact)\n",
        "print(\"\\nAbsolute difference |x - x_exact|:\\n\", abs_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od0psVpKgX8e"
      },
      "source": [
        "\n",
        "The procedure to compute the distance of a point $\\mathbf{x}_0 \\in \\mathbb{R}^n$ to\n",
        "an elliptic surface $\\mathbf{x}^\\intercal \\mathbf{A} \\mathbf{x} = 1$, where $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite ($\\mathbf{A} \\succ 0$) can be derived as follows:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kDdMYE9gp2m"
      },
      "source": [
        "\n",
        " $\\mathbf{x}^\\intercal \\mathbf{A} \\mathbf{x} = 1$  \n",
        "\n",
        " $$  \\text{Distance function (squared distance):}\\\\\n",
        "d^2 = \\|\\mathbf{x} - \\mathbf{x}_0\\|^2 $$\n",
        "\n",
        "$$\\text{Constraint (ellipsoid surface):} \\\\\n",
        " \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} - 1 = 0\n",
        "$$\n",
        "\n",
        "Thus:   \n",
        "\n",
        "$ ∇(\\|\\mathbf{x} - \\mathbf{x}_0\\|^2) = λ ∇(\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} - 1)$\n",
        "\n",
        "$ ∇ \\|\\mathbf{x} - \\mathbf{x}_0\\|^2 = ∇((\\mathbf{x} - \\mathbf{x}_0))^\\top (\\mathbf{x} - \\mathbf{x}_0)   $\n",
        "\n",
        "$ =∇ \\big[ \\mathbf{x}^\\top \\mathbf{x} - \\mathbf{x}^\\top \\mathbf{x}_0 - \\mathbf{x}_0^\\top \\mathbf{x} + \\mathbf{x}_0^\\top \\mathbf{x}_0 \\big] $\n",
        "\n",
        "$ =∇ \\big[ \\mathbf{x}^\\top \\mathbf{x} -2 \\mathbf{x}^\\top \\mathbf{x}_0 + \\mathbf{x}_0^\\top \\mathbf{x}_0 \\big] $\n",
        "\n",
        "$ = 2x  -2\\mathbf{x}_0 $\n",
        "\n",
        "$∇(\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} - 1) = 2Ax \\quad \\text{(as A is symmetric)}  $\n",
        "\n",
        "\n",
        "Applying Lagrange's Theorem:\n",
        "\n",
        "$2(x  -\\mathbf{x}_0) = λ(2Ax) \\\\\n",
        "x -\\mathbf{x}_0 = λAx  \\\\\n",
        "(x - \\lambda \\mathbf{A}x) = \\mathbf{x}_0 \\\\\n",
        "(\\mathbf{I} - \\lambda \\mathbf{A}) \\mathbf{x} = \\mathbf{x}_0 \\\\\n",
        "\\mathbf{x}^* = (\\mathbf{I} - \\lambda^* \\mathbf{A})^{-1} \\mathbf{x}_0$\n",
        "\n",
        "Substituting into the constraint:\n",
        "\n",
        "$$ \\Big( (\\mathbf{I} - \\lambda^* \\mathbf{A})^{-1} \\mathbf{x}_0 \\Big)^\\top\n",
        "\\mathbf{A} \\Big( (\\mathbf{I} - \\lambda^* \\mathbf{A})^{-1} \\mathbf{x}_0 \\Big) - 1 = 0 $$  \n",
        "\n",
        "\n",
        "\n",
        "**General Numerical Solution for $\\lambda^*$:**\n",
        "\n",
        "Define the function (with f(λ) = 0):\n",
        "\n",
        "$$\n",
        "f(\\lambda) = 0 = \\big((\\mathbf{I} - \\lambda \\mathbf{A})^{-1} \\mathbf{x}_0\\big)^\\top\n",
        "\\mathbf{A} \\big((\\mathbf{I} - \\lambda \\mathbf{A})^{-1} \\mathbf{x}_0\\big) - 1\n",
        "$$\n",
        "\n",
        "Compute its derivative:\n",
        "\n",
        "$$\n",
        "f’(\\lambda) = 2\\mathbf{y}^\\top \\mathbf{A} (\\mathbf{I} - \\lambda \\mathbf{A})^{-1} \\mathbf{A}\n",
        "$$\n",
        "\n",
        "Newton method:\n",
        "\n",
        "$$\n",
        "\\lambda_{k+1} = \\lambda_k - \\frac{f(\\lambda_k)}{f’(\\lambda_k)}\n",
        "$$\n",
        "\n",
        "Repeat until:\n",
        "\n",
        "$$\n",
        "|f(\\lambda_k)| < \\text{tolerance}\n",
        "$$\n",
        "\n",
        "Once converged, compute the closest point:\n",
        "\n",
        "$$\n",
        "\\mathbf{x}^* = (\\mathbf{I} - \\lambda^* \\mathbf{A})^{-1} \\mathbf{x}_0\n",
        "$$\n",
        "\n",
        "Distance:\n",
        "\n",
        "$$\n",
        "d = |\\mathbf{x}^* - \\mathbf{x}_0|\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cwqbh2ugrgY"
      },
      "source": [
        "I implemented a grid search using numpy to compute the distance from a given point $ \\mathbf{x}_0 $ to the elliptic surface defined by $$\\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} = 1$$ Using the provided $\\mathbf{x}_0$ and $\\mathbf{A}$ as an example, I performed the search over candidate points on the surface and computed the corresponding distances to ultimately identify and print the minimum distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97-9gK9shLMy",
        "outputId": "c4e887c8-d8db-4fd3-f2f6-28ea7eec068f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A.shape =  (5, 5)\n",
            "x0.shape =  (5,)\n",
            "Grid Search Results:\n",
            "lambda* = -0.93219932199322\n",
            "x* = [ 0.0476721   0.18436309  0.19580546  0.39958587 -0.13646972]\n",
            "distance = 2.120256195365985\n",
            "Ellipsoid constraint (≈1): 1.0000001294457894\n",
            "\n",
            "Newton's Method Results:\n",
            "lambda* = -0.9321993974452482\n",
            "x* = [ 0.0476721   0.18436308  0.19580545  0.39958585 -0.13646972]\n",
            "distance = 2.1202562238222824\n",
            "Ellipsoid constraint (≈1): 0.9999999999999997\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import root_scalar\n",
        "\n",
        "A = np.array([[10.82133645,3.92407517,0.28991746,-2.03411727,4.55376106],\n",
        " [3.92407517,8.55336068,-0.21915151,0.42212004,2.08124307],\n",
        " [0.28991746,-0.21915151,5.74622299,-0.800496,0.63366946],\n",
        " [-2.03411727,0.42212004,-0.800496,3.91232017,-0.44196245],\n",
        " [4.55376106,2.08124307,0.63366946,-0.44196245,3.83040884]])\n",
        "\n",
        "x0 = np.array([-0.08111786,1.68122205,0.84108852,1.74916213,-0.11266981])\n",
        "\n",
        "print(\"A.shape = \", A.shape)\n",
        "print(\"x0.shape = \", x0.shape)\n",
        "\n",
        "def f(lam):\n",
        "    I = np.eye(A.shape[0])\n",
        "    x = np.linalg.solve(I - lam*A, x0)\n",
        "    return x.T @ A @ x - 1\n",
        "\n",
        "lam_values = np.linspace(-1.0, 1.0, 100000)\n",
        "f_vals = []\n",
        "\n",
        "for lam in lam_values:\n",
        "    try:\n",
        "        val = f(lam)\n",
        "        f_vals.append(np.abs(val))\n",
        "    except np.linalg.LinAlgError:\n",
        "        f_vals.append(np.inf)\n",
        "\n",
        "f_vals = np.array(f_vals)\n",
        "idx_min = np.argmin(f_vals)\n",
        "lambda_star = lam_values[idx_min]\n",
        "\n",
        "# Closest point and distance\n",
        "x_star = np.linalg.solve(np.eye(A.shape[0]) - lambda_star*A, x0)\n",
        "distance = np.linalg.norm(x_star - x0)\n",
        "\n",
        "print(\"Grid Search Results:\")\n",
        "print(\"lambda* =\", lambda_star)\n",
        "print(\"x* =\", x_star)\n",
        "print(\"distance =\", distance)\n",
        "print(\"Ellipsoid constraint (≈1):\", x_star.T @ A @ x_star)\n",
        "print()\n",
        "\n",
        "def fprime(lam):\n",
        "    I = np.eye(A.shape[0])\n",
        "    inv = np.linalg.inv(I - lam*A)\n",
        "    x = inv @ x0\n",
        "    return 2 * x.T @ A @ (inv @ (A @ x))\n",
        "\n",
        "# Apply Newton’s method for finding roots\n",
        "sol = root_scalar(f, fprime=fprime, x0=0.0, method='newton')\n",
        "lambda_newton = sol.root\n",
        "\n",
        "# Apply Newton’s method for finding roots\n",
        "x_star_newton = np.linalg.solve(np.eye(A.shape[0]) - lambda_newton*A, x0)\n",
        "distance_newton = np.linalg.norm(x_star_newton - x0)\n",
        "\n",
        "print(\"Newton's Method Results:\")\n",
        "print(\"lambda* =\", lambda_newton)\n",
        "print(\"x* =\", x_star_newton)\n",
        "print(\"distance =\", distance_newton)\n",
        "print(\"Ellipsoid constraint (≈1):\", x_star_newton.T @ A @ x_star_newton)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
